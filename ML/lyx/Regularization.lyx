#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{fancyvrb}
\usepackage{color}
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.25,0.82}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.50,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.44,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.38,0.00,0.88}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.31,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.19,0.38,0.56}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.82}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.50,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.31,0.44,0.56}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.56,0.44,0.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.19,0.38}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.75}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.38}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.31,0.31,0.31}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.94,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.38,0.69}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{0.88,0.88,0.88}{\strut ##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{1.00,0.94,0.94}{\strut ##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.19,0.19,0.69}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.44,0.00}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.56,0.38,0.19}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{1.00,0.94,0.94}{\strut ##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{1.00,0.94,0.94}{\strut ##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.82,0.13,0.00}{##1}}\def\PY@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{1.00,0.94,0.94}{\strut ##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.50,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.50,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.38,0.00,0.88}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.94,0.00,0.00}{##1}}\def\PY@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{0.94,0.63,0.63}{\strut ##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.38,0.00}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.00}{##1}}\def\PY@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{1.00,0.94,1.00}{\strut ##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.25,0.00,0.88}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.82}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.19,0.19,0.19}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{1.00,0.94,0.94}{\strut ##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.19,0.50}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.19,0.19,0.56}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.25,0.82}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{1.00,0.94,0.94}{\strut ##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.38,0.38,0.38}{##1}}\def\PY@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{1.00,0.94,0.94}{\strut ##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.13}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\@addtoreset{section}{part}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation 0bp
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Regularization
\end_layout

\begin_layout Section
The problem of overfitting.
\end_layout

\begin_layout Itemize

\series bold
Underfitting
\series default
 - 
\bar under
High bias
\bar default
 - doesn't fit the data very well
\end_layout

\begin_layout Itemize

\series bold
Overfitting
\series default
 - 
\bar under
High variance
\bar default
 - high order polynomial, fits data very well, space of possible hypothesis
 is too variable to give a good one
\end_layout

\begin_deeper
\begin_layout Itemize
If we have too many features, the learned hypothesis may fit the training
 set very well (
\begin_inset Formula $J(\Theta)\approx0)$
\end_inset

, but fail to generalize to new examples (predict prices on new examples)
\end_layout

\end_deeper
\begin_layout Subsection
Addressing overfitting
\end_layout

\begin_layout Standard
Lots of features and little training data, overfitting can occur.
\end_layout

\begin_layout Standard
If we think overfitting is occuring, what can we do?
\end_layout

\begin_layout Itemize
Plotting hypothesis - to select an appropriate degree polynomial.
 Doesn't always work, you can have too many features.
\end_layout

\begin_layout Itemize
Reduce number of features.
\end_layout

\begin_deeper
\begin_layout Itemize
Manually select which features to keep.
\end_layout

\begin_layout Itemize
Model selection algorithm
\end_layout

\end_deeper
\begin_layout Itemize
Regularization.
\end_layout

\begin_deeper
\begin_layout Itemize
Keep all features, but reduce magnitude/values of parameters 
\begin_inset Formula $\Theta_{j}$
\end_inset

.
\end_layout

\begin_layout Itemize
Works well when we have a 
\bar under
lot of features
\bar default
, each of which contributes a bit to predicting 
\begin_inset Formula $y$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Section
Cost function
\end_layout

\begin_layout Standard
Take parameters:
\end_layout

\begin_layout Standard
\begin_inset Formula $\Theta_{0}+\Theta_{1}x+\Theta_{2}x^{2}\rightarrow$
\end_inset

just right
\end_layout

\begin_layout Standard
\begin_inset Formula $\Theta_{0}+\Theta_{1}x+\Theta_{2}x^{2}+\Theta_{3}x^{3}+\Theta_{4}x^{4}\rightarrow$
\end_inset

overfitted
\end_layout

\begin_layout Standard
Suppose we penalize and make 
\begin_inset Formula $\Theta_{3},\Theta_{4}$
\end_inset

really small.
\end_layout

\begin_layout Standard
\begin_inset Formula ${\displaystyle \min_{\Theta}\frac{1}{2m}{\displaystyle {\displaystyle \sum_{i=1}^{m}(h_{\Theta}(x^{(i)})-y^{(i)})^{2}}}}+1000\Theta_{3}^{2}+1000\Theta_{4}^{2}$
\end_inset


\end_layout

\begin_layout Standard
The only way to make this cost function small is when 
\begin_inset Formula $\Theta_{3}\approx0\, and\,\Theta_{4}\approx0$
\end_inset

 and thus essentially getting rid of the high order terms.
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Standard
In general:
\end_layout

\begin_layout Standard
Small values for parameters 
\begin_inset Formula $\Theta_{0},\Theta_{1},\ldots,\Theta_{n}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Simpler
\begin_inset Quotes erd
\end_inset

 hypothesis
\end_layout

\begin_layout Itemize
Less prone to overfitting
\end_layout

\begin_layout Standard
Ex: Housing:
\end_layout

\begin_layout Standard
Features: 
\begin_inset Formula $x_{1},x_{2},\ldots,x_{100}$
\end_inset

- hard to pick in advance which are the ones that are less likely to be
 relevant
\end_layout

\begin_layout Standard
Parameters: 
\begin_inset Formula $\Theta_{0},\Theta_{1},\ldots,\Theta_{100}$
\end_inset


\end_layout

\begin_layout Standard
We'll take the cost function and add a regularization term at the end to
 shrink all of the parameters.
 Not the 
\begin_inset Formula $\Theta_{0}$
\end_inset

term
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
J(\Theta)=\frac{1}{2m}{\displaystyle \left[\sum_{i=1}^{m}(h_{\Theta}(x^{(i)})-y^{(i)})^{2}+\lambda{\displaystyle \sum_{j=1}^{n}\Theta_{j}^{2}}\right]}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\lambda$
\end_inset

- regularization parameter, controls a tradeoff between two different goals:
\end_layout

\begin_layout Itemize
we would like to train the training set well (first term)
\end_layout

\begin_layout Itemize
we want to keep the parameters small (regularization term) and therefore
 keeping the hypothesis simple to avoid overfitting
\end_layout

\begin_layout Standard
What if 
\begin_inset Formula $\lambda$
\end_inset

is set to an extremely large value (perhaps too large for our problem, say
 
\begin_inset Formula $\lambda=10^{10}$
\end_inset

)?
\end_layout

\begin_layout Standard
We would end up penalizing all the parameters so they end up close to zero,
 ending up with a hypothesis: 
\begin_inset Formula 
\[
h_{\Theta}(x)=\Theta_{0}
\]

\end_inset


\end_layout

\begin_layout Standard
and underfit the training set.
 Hypothesis has too high bias or preconception.
\end_layout

\begin_layout Section
Regularized linear regression
\end_layout

\begin_layout Standard
For linear regression we have two algorithms
\end_layout

\begin_layout Itemize
Gradient descent
\end_layout

\begin_layout Itemize
Normal equation
\end_layout

\begin_layout Standard
We'll generalize them for linear regression.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
J(\Theta)=\frac{1}{2m}{\displaystyle \left[\sum_{i=1}^{m}(h_{\Theta}(x^{(i)})-y^{(i)})^{2}+\lambda{\displaystyle \sum_{j=1}^{n}\Theta_{j}^{2}}\right]}
\]

\end_inset


\end_layout

\begin_layout Subsection
Gradient descent
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
Repeat\,\{\\
 & \Theta_{j}:=\Theta_{j}-\alpha\frac{1}{m}{\displaystyle \sum_{i=1}^{m}(h_{\Theta}(x)^{(i)}-y^{(i)})x_{j}^{(i)}}\\
 & (j=0,1,2,3,\ldots,n)\\
\}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
With regularization for linear regression:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
Repeat\,\{\\
 & \Theta_{0}:=\Theta_{0}-\alpha\frac{1}{m}{\displaystyle \sum_{i=1}^{m}(h_{\Theta}(x)^{(i)}-y^{(i)})x_{0}^{(i)}}\\
 & \Theta_{j}:=\Theta_{j}-\alpha\frac{1}{m}{\displaystyle \sum_{i=1}^{m}(h_{\Theta}(x)^{(i)}-y^{(i)})x_{j}^{(i)}+\frac{\lambda}{m}\Theta_{j}}\\
\} & (j=1,2,3,\ldots,n)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The update for 
\begin_inset Formula $\Theta_{j}$
\end_inset

can be written equivalently as
\begin_inset Formula 
\[
\Theta_{j}:=\Theta_{j}(1-\alpha\frac{\lambda}{m})-\alpha\frac{1}{m}{\displaystyle \sum_{i=1}^{m}(h_{\Theta}(x)^{(i)}-y^{(i)})x_{j}^{(i)}}
\]

\end_inset


\end_layout

\begin_layout Standard
The first term, 
\begin_inset Formula $1-\alpha\frac{1}{m}<1$
\end_inset

, has an effect of shrinking 
\begin_inset Formula $\Theta_{j}$
\end_inset

and then the second term is the same as original gradient descent update.
\end_layout

\begin_layout Subsection
Normal equation
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
X=\left[\begin{array}{c}
(x^{(1)})^{\top}\\
\vdots\\
(x^{(m)})^{\top}
\end{array}\right] &  & y=\left[\begin{array}{c}
y^{(1)}\\
\\
y^{(m)}
\end{array}\right]\in\mathbb{R}^{m}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\Theta=(X^{\top}X+\lambda\left[\begin{array}{ccccc}
0 & 0 & 0 & 0 & 0\\
0 & 1 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & \ddots & \vdots\\
0 & 0 & 0 & \dots & 1
\end{array}\right])^{-1}X^{\top}y
\]

\end_inset


\end_layout

\begin_layout Standard
The inner matrix has dimensions: 
\begin_inset Formula $(n+1)\times(n+1)$
\end_inset


\end_layout

\begin_layout Subsubsection
Non-invertibility
\end_layout

\begin_layout Standard
Suppose 
\begin_inset Formula $m\leq n$
\end_inset

, (# examples < #features)
\end_layout

\begin_layout Standard
pinv will give an example that kindof makes sense, numerically correct but
 not necessarily a good hypothesis.
\end_layout

\begin_layout Standard
if 
\begin_inset Formula $\lambda>0$
\end_inset

, it is possible to prove that term of
\begin_inset Formula 
\[
\Theta=(X^{\top}X+\lambda\left[\begin{array}{ccccc}
0 & 0 & 0 & 0 & 0\\
0 & 1 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & \ddots & \vdots\\
0 & 0 & 0 & \dots & 1
\end{array}\right])^{-1}X^{\top}y
\]

\end_inset

 in parentheses will be invertible.
\end_layout

\begin_layout Section
Regularized Logistic Regression
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
J(\Theta)=-\left[\frac{1}{m}\sum_{i=1}^{m}y^{(i)}\log h_{\Theta}(x^{(i)})+(1-y^{(i)})\log(1-h_{\Theta}(x^{(i)}))\right]+\lambda{\displaystyle \sum_{j=1}^{n}\Theta_{j}^{2}}
\]

\end_inset


\end_layout

\begin_layout Standard
So now, even if we're fitting a very high order polynomial, we'll more likely
 get a decision boundary that looks reasonable.
\end_layout

\begin_layout Standard
Implementation:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
Repeat\,\{\\
 & \Theta_{0}:=\Theta_{0}-\alpha\frac{1}{m}{\displaystyle \sum_{i=1}^{m}(h_{\Theta}(x)^{(i)}-y^{(i)})x_{0}^{(i)}}\\
 & \Theta_{j}:=\Theta_{j}-\alpha\left[\frac{1}{m}{\displaystyle \sum_{i=1}^{m}(h_{\Theta}(x)^{(i)}-y^{(i)})x_{j}^{(i)}+\frac{\lambda}{m}\Theta_{j}}\right]\\
\} & (j=1,2,3,\ldots,n)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The difference from linear regression is that the hypothesis is different
\begin_inset Formula 
\[
h_{\Theta}(x)=\frac{1}{1+e^{-\Theta^{\top}x}}
\]

\end_inset


\end_layout

\begin_layout Subsection
Advanced optimization
\end_layout

\begin_layout Standard
function [jVal, gradient] = costFunction(theta)
\end_layout

\begin_layout Standard
% below code now must include the regularization parameter
\end_layout

\begin_layout Standard
jval = [code to compute 
\begin_inset Formula $J(\Theta)$
\end_inset

];
\end_layout

\begin_layout Standard
gradient(1) = [code to compute 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\frac{\partial}{\partial\Theta_{0}}J(\Theta)$
\end_inset

];
\end_layout

\begin_layout Standard
gradient(2) = [code to compute 
\begin_inset Formula $\frac{\partial}{\partial\Theta_{1}}J(\Theta)$
\end_inset

];
\end_layout

\begin_layout Standard
\begin_inset Formula $\vdots$
\end_inset


\end_layout

\begin_layout Standard
gradient(n+1) = [code to compute 
\begin_inset Formula $\frac{\partial}{\partial\Theta_{n}}J(\Theta)$
\end_inset

];
\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Standard
Then you pass the results to fminunc or other advanced optimization algorithm,
 the params you get out will correspond to regularized logistic regression.
\end_layout

\end_body
\end_document
