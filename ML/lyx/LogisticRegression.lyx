#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{color}
\definecolor{hellgelb}{rgb}{1,1,0.85}
\definecolor{colKeys}{rgb}{0,0,1}
\definecolor{colIdentifier}{rgb}{0,0,0}
\definecolor{colComments}{rgb}{1,0,0}
\definecolor{colString}{rgb}{0,0.5,0}
\lstset{%
     language=Matlab,%
     float=hbp,%
     basicstyle=\footnotesize\ttfamily,%
     identifierstyle=\color{colIdentifier},%
     keywordstyle=\color{colKeys},%
     stringstyle=\color{colString},%
     commentstyle=\itshape\color{colComments},%
     columns=fixed,
     tabsize=4,%
     frame=single,%
     framerule=1pt,
     extendedchars=true,%
     showspaces=false,%
     showstringspaces=false,%
     numbers=left,%
     numberstyle=\tiny\ttfamily,%
     numbersep=1em,%
     breaklines=true,%
     breakindent=10pt,%
     backgroundcolor=\color{hellgelb},%
     breakautoindent=true,%
     captionpos=t,%
     xleftmargin=1em,%
     xrightmargin=\fboxsep%
}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation 0bp
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Logistic Regression
\end_layout

\begin_layout Section
Logistic Regression
\end_layout

\begin_layout Subsection
\noindent
Decision Boundary
\end_layout

\begin_layout Standard
Decision boundary is a property of the hypothesis and not the data set.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
h_{\Theta}(x)=g(\Theta^{\top}x)=P(y=1|x;\Theta)
\]

\end_inset


\end_layout

\begin_layout Standard
sigmoid function:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
g(z)=\frac{1}{1+e^{-z}}
\]

\end_inset


\end_layout

\begin_layout Standard
suppose predict 
\begin_inset Formula $y=1$
\end_inset

if
\begin_inset Formula $h_{\Theta}(x)\ge0.5$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
g(z)\ge0.5\, when\, z\ge0
\]

\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset Formula 
\[
h_{\Theta}(x)=g(\Theta^{\top}x)\ge0.5\, whenever\,\Theta^{\top}x\ge0
\]

\end_inset


\end_layout

\begin_layout Standard
suppose predict 
\begin_inset Formula $y=0$
\end_inset

if 
\begin_inset Formula $h_{\Theta}(x)<0.5$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
h_{\Theta}(x)=g(\Theta^{\top}x)<0.5\, whenever\,\Theta^{\top}x<0
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
Non Linear Decision Boundaries
\end_layout

\begin_layout Standard
Add higher order polynomials, 2 exra features.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
h_{\Theta}(x)=g(\Theta_{0}+\Theta_{1}x_{1}+\Theta_{2}x_{2}+\Theta_{3}x_{1}^{2}+\Theta_{4}x_{2}^{2})
\]

\end_inset


\begin_inset Formula 
\[
\Theta=\left[\begin{array}{c}
-1\\
0\\
0\\
1\\
1
\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
predict 
\begin_inset Formula $y=1$
\end_inset

if
\begin_inset Formula $-1+x_{1}^{2}+x_{2}^{2}\ge0$
\end_inset

, that is 
\begin_inset Formula $x_{1}^{2}+x_{2}^{2}\ge1$
\end_inset


\end_layout

\begin_layout Standard
if we plot 
\begin_inset Formula $x_{1}^{2}+x_{2}^{2}=1$
\end_inset

, it's a circle.
 Inside of the circle, 
\begin_inset Formula $y=0$
\end_inset

, outside 
\begin_inset Formula $y=1$
\end_inset

.
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Standard
With even higher polynomial terms, we can get even more complex decision
 boundaries.
\end_layout

\begin_layout Subsection
Cost Function
\end_layout

\begin_layout Standard
Training set: 
\begin_inset Formula 
\[
\{(x^{(1)},\, y^{(1)}),\,(x^{(2)},y^{(2)}),\,...,\,(x^{(m)},y^{(m)})
\]

\end_inset


\end_layout

\begin_layout Standard
m examples:
\begin_inset Formula 
\[
x\in\left[\begin{array}{c}
x_{0}\\
x_{1}\\
...\\
x_{n}
\end{array}\right]\in\mathbb{R}^{n+1}\,\,\, x_{0}=1,\, y\in\left\{ 0,1\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
h_{\Theta}(x)=\frac{1}{1+e^{-\Theta^{\top}x}}
\]

\end_inset


\end_layout

\begin_layout Standard
How do we choose parameters 
\begin_inset Formula $\Theta$
\end_inset

?
\end_layout

\begin_layout Standard
Cost function for logistic regression, needs to be convex and a cost function
 for linear regression would not be convex.
 Instead, use (6:20):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Cost(h_{\Theta}(x),y)=\begin{cases}
-log(h_{\Theta}(x)) & if\, y=1\\
-log(1-h_{\Theta}(x)) & if\, y=0
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $Cost=0\, if\, y=1,\, h_{\Theta}(x)=1$
\end_inset


\end_layout

\begin_layout Standard
But as 
\begin_inset Formula $h_{\Theta}(x)\rightarrow0,\, Cost\rightarrow\infty$
\end_inset


\end_layout

\begin_layout Standard
Captures intuition that if 
\begin_inset Formula $h_{\Theta}(x)=0$
\end_inset

, (predict 
\begin_inset Formula $P(y=1|x;\Theta)$
\end_inset

, but 
\begin_inset Formula $y=1$
\end_inset

, we'll penalize learning algorithm by a very large cost.
\end_layout

\begin_layout Subsection
Simplified cost function and gradient descent
\end_layout

\begin_layout Standard

\series bold
Logistic regression cost function
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
J(\Theta)=\frac{1}{m}{\displaystyle \sum_{i=1}^{m}Cost(h_{\Theta}(x^{(i)},y^{(i)})}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Cost(h_{\Theta}(x),y)=\begin{cases}
-log(h_{\Theta}(x)) & if\, y=1\\
-log(1-h_{\Theta}(x)) & if\, y=0
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard
Note: 
\begin_inset Formula $y=0\, or\,1$
\end_inset

always
\end_layout

\begin_layout Standard
We can rewrite the cost function:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Cost(h_{\Theta}(x),y)=-y\log(h_{\Theta}(x))-(1-y)\log(1-h_{\Theta}(x))
\]

\end_inset


\end_layout

\begin_layout Standard
So then if 
\begin_inset Formula $y=1$
\end_inset

or if 
\begin_inset Formula $y=0$
\end_inset

, this function loses terms accordingly.
\end_layout

\begin_layout Standard
Now we can rewrite the cost function like so:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
J(\Theta)=-\frac{1}{m}\left[\sum_{i=1}^{m}y^{(i)}\log h_{\Theta}(x^{(i)})+(1-y^{(i)})\log(1-h_{\Theta}(x^{(i)}))\right]
\]

\end_inset


\end_layout

\begin_layout Standard
To fit parameters 
\begin_inset Formula $\Theta$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\min_{\Theta}J(\Theta)
\]

\end_inset


\end_layout

\begin_layout Standard
To make a prediction given new 
\begin_inset Formula $x$
\end_inset

:
\end_layout

\begin_layout Standard
Output 
\begin_inset Formula 
\[
h_{\Theta}(x)=\frac{1}{1+e^{-\Theta^{\top}x}}
\]

\end_inset


\end_layout

\begin_layout Standard
which we interpret as probability that 
\begin_inset Formula $y=1$
\end_inset


\begin_inset Formula 
\[
p(y=1|x;\Theta)
\]

\end_inset


\end_layout

\begin_layout Standard
We're going to minimize the cost function with 
\series bold
Gradient Descent.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\min_{\Theta}J(\Theta):\, Repeat\,\{\\
 & \Theta_{j}=\Theta_{j}-\alpha{\displaystyle \sum_{i=1}^{m}(h_{\Theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}}\\
\}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\Theta=\left[\begin{array}{c}
\Theta_{0}\\
\Theta_{1}\\
\vdots\\
\Theta_{n}
\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
Vectorized:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\Theta:=\Theta-\alpha{\displaystyle \sum_{i=1}^{m}(h_{\Theta}(x^{(i)})-y^{(i)})x^{(i)}}
\]

\end_inset


\end_layout

\begin_layout Standard
This is not the same as Gradient Descent for linear regression, because
 the definition of 
\begin_inset Formula $h_{\Theta}(x)$
\end_inset

has changed.
\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Standard
Apply the method from linear regression to make sure logistic regression
 is converging, plot cost.
\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Standard
Note:
\end_layout

\begin_layout Standard
Recall:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
J(\Theta)=-\frac{1}{m}\left[\sum_{i=1}^{m}y^{(i)}\log h_{\Theta}(x^{(i)})+(1-y^{(i)})\log(1-h_{\Theta}(x^{(i)}))\right]
\]

\end_inset


\end_layout

\begin_layout Standard
Vectorized:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
J(\Theta)=\frac{1}{m}\left[-y^{\top}\log h_{\Theta}(X)-(1-y)^{\top}\log(1-h_{\Theta}(X))\right]
\]

\end_inset


\end_layout

\begin_layout Standard
The gradient of the cost is a vector of the same length as 
\begin_inset Formula $\Theta$
\end_inset

where the 
\begin_inset Formula $j^{th}$
\end_inset

element for 
\begin_inset Formula $j=(0,1,2,\dots,n)$
\end_inset

 is defined as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial J(\Theta)}{\partial\Theta_{j}}=\frac{1}{m}{\displaystyle \sum_{i=1}^{m}(h_{\Theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}}
\]

\end_inset


\end_layout

\begin_layout Standard
Vectorized:
\begin_inset Formula 
\[
\frac{\partial J(\Theta)}{\partial\Theta_{j}}=\frac{1}{m}X^{\top}h_{\Theta}(X)-y
\]

\end_inset


\end_layout

\begin_layout Subsection
Advanced Optimization
\end_layout

\begin_layout Standard
Given 
\begin_inset Formula $\Theta$
\end_inset

, we have code that can compute:
\end_layout

\begin_layout Itemize
\begin_inset Formula $J(\Theta)$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\frac{\partial}{\partial\Theta_{j}}J(\Theta)$
\end_inset

 (for 
\begin_inset Formula $j=0,1,\ldots,n)$
\end_inset


\end_layout

\begin_layout Standard
We can use these to minimize the cost function:
\end_layout

\begin_layout Itemize
Gradient descent
\end_layout

\begin_layout Itemize
Conjugate gradient
\end_layout

\begin_layout Itemize
BFGS
\end_layout

\begin_layout Itemize
L-BFGS
\end_layout

\begin_layout Standard
Advantages:
\end_layout

\begin_layout Itemize
No need to manually pick 
\begin_inset Formula $\alpha$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Line search algorithm automatically picks 
\begin_inset Formula $\alpha$
\end_inset

, even for each iteration.
\end_layout

\end_deeper
\begin_layout Itemize
Often faster than gradient descent
\end_layout

\begin_layout Standard
Disadvantages:
\end_layout

\begin_layout Itemize
More complex
\end_layout

\begin_layout Standard
Example:
\end_layout

\begin_layout Standard
\begin_inset Formula $\Theta=\left[\begin{array}{c}
\Theta_{1}\\
\Theta_{2}
\end{array}\right]$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\Theta_{1}=5$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\Theta_{2}=5$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $J(\Theta)=(\Theta_{1}-5)^{2}+(\Theta_{2}-5)^{2}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\frac{\partial}{\partial\Theta_{1}}J(\Theta)=2(\Theta_{1}-5)$
\end_inset


\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula $\frac{\partial}{\partial\Theta_{2}}J(\Theta)=2(\Theta_{1}-5)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

function [jVal, gradient] = costFunction(theta)
\end_layout

\begin_layout Plain Layout

	jVal = (theta(1) - 5)^2 + (theta(2) - 5)^2;
\end_layout

\begin_layout Plain Layout

	gradient = zeros(2, 1)
\end_layout

\begin_layout Plain Layout

	gradient(1) = 2 * (theta(1) - 5);
\end_layout

\begin_layout Plain Layout

	gradient(1) = 2 * (theta(2) - 5);
\end_layout

\end_inset


\end_layout

\begin_layout Standard
jVal gets the result of the cost function
\end_layout

\begin_layout Standard
gradient gets a 2x1 vector, the terms of which correspond to the two derivative
 terms.
\end_layout

\begin_layout Standard
We can then call the advanced optimization function, fminunc, function minimizat
ion unconstrained.
 Chooses optimal value of 
\begin_inset Formula $\Theta$
\end_inset

automagically.
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

options = optimset('GradObj', 'on', 'MaxIter', '100');
\end_layout

\begin_layout Plain Layout

initialTheta = zeros(2, 1);
\end_layout

\begin_layout Plain Layout

[optTheta, functionVal, exitFlag] = ...
\end_layout

\begin_layout Plain Layout

	fminunc(@costFunction, initialTheta, options)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
@ represents a pointer to the above costFunction
\end_layout

\begin_layout Standard
set some options:
\end_layout

\begin_layout Itemize
'GradObj', 'on' means that we will provide a gradient
\end_layout

\begin_layout Itemize
'MaxIter', '100' means the number of iterations
\end_layout

\begin_layout Standard
\begin_inset Formula $initialTheta\in\mathbb{R}^{d},\, d\ge2$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Standard

\series bold
How do we apply this to logistic regression?
\end_layout

\begin_layout Standard
\begin_inset Formula $theta=\left[\begin{array}{c}
\Theta_{0}\\
\Theta_{1}\\
\vdots\\
\Theta_{n}
\end{array}\right]$
\end_inset


\end_layout

\begin_layout Standard
function [jVal, gradient] = costFunction(theta)
\end_layout

\begin_layout Standard
jval = [code to compute 
\begin_inset Formula $J(\Theta)$
\end_inset

];
\end_layout

\begin_layout Standard
gradient(1) = [code to compute 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\frac{\partial}{\partial\Theta_{0}}J(\Theta)$
\end_inset

];
\end_layout

\begin_layout Standard
gradient(2) = [code to compute 
\begin_inset Formula $\frac{\partial}{\partial\Theta_{1}}J(\Theta)$
\end_inset

];
\end_layout

\begin_layout Standard
\begin_inset Formula $\vdots$
\end_inset


\end_layout

\begin_layout Standard
gradient(n+1) = [code to compute 
\begin_inset Formula $\frac{\partial}{\partial\Theta_{n}}J(\Theta)$
\end_inset

];
\end_layout

\begin_layout Subsection
Multi-class classification: One-vs-all
\end_layout

\begin_layout Standard
How to get logistic regression to get to work with multi-class classification
 problems.
\end_layout

\begin_layout Standard
Scenario 1: tag emails for different folders
\end_layout

\begin_layout Standard
Scenario 2: Medical diagrams: Not ill, cold, flu
\end_layout

\begin_layout Standard
Scenario 3: Weather: Sunny, Cludy, Rainy, Snow
\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Standard
There will be more groups of items on plot.
\end_layout

\begin_layout Standard
Turn the problem into 3 different binary classification problems.
\end_layout

\begin_layout Standard
We'll create a 
\begin_inset Quotes eld
\end_inset

fake
\begin_inset Quotes erd
\end_inset

 trainig set, where Class 1 gets assigned to a positive class and Class
 2 and 3 get assigned to the negative class.
 
\end_layout

\begin_layout Standard
\begin_inset Formula $h_{\Theta}^{(1)}(x)$
\end_inset

will denote, say triangles
\end_layout

\begin_layout Standard
\begin_inset Formula $h_{\Theta}^{(2)}(x)$
\end_inset

will denote, say squares
\end_layout

\begin_layout Standard
\begin_inset Formula $h_{\Theta}^{(2)}(x)$
\end_inset

will denote, say x's
\end_layout

\begin_layout Standard
We fit 3 classifiers that try to estimate what is the probability that 
\begin_inset Formula $y=i$
\end_inset


\begin_inset Formula 
\[
h_{\Theta}^{(i)}=P(y=i|x;\Theta)\,(i=1,2,3)
\]

\end_inset


\end_layout

\begin_layout Standard
Train a logistic regression classifier 
\begin_inset Formula $h_{\Theta}^{(i)}x$
\end_inset

 for each class 
\begin_inset Formula $i$
\end_inset

 to predict the probability that 
\begin_inset Formula $y=1$
\end_inset

.
\end_layout

\begin_layout Standard
On a new input 
\begin_inset Formula $x$
\end_inset

, to make a prediction, pick the class 
\begin_inset Formula $i$
\end_inset

 that maximizes
\begin_inset Formula 
\[
\max_{i}h_{\Theta}^{(i)}(x)
\]

\end_inset


\end_layout

\end_body
\end_document
